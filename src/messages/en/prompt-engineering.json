{
  "promptEngineering": {
    "coreConceptsTitle": "Core Concepts",
    "title": "Prompt Engineering Mastery",
    "description": "Learn practical prompt engineering techniques through interactive lessons and hands-on exercises",
    "startLearning": "Start Learning",
    "viewLessons": "View Lessons",
    "lessonsLabel": "Lessons",
    "interactiveLearning": "Interactive Learning",
    "learningPath": "Learning Path",
    "learningPathDescription": "Master prompt engineering through structured lessons and practical exercises",
    "backToLessons": "Back to Lessons",
    "lesson": "Lesson",
    "chapter": "Chapter",
    "theory": "Theory",
    "examples": "Examples",
    "exercises": "exercises",
    "practicalExamples": "Practical Examples",
    "prerequisites": "Prerequisites",
    "exercise": "Exercise",
    "instructions": "Instructions",
    "hints": "Hints",
    "showHints": "Show Hints",
    "hideHints": "Hide Hints",
    "yourPrompt": "Your Prompt",
    "enterPromptHere": "Enter your prompt here...",
    "checkAnswer": "Check Answer",
    "reset": "Reset",
    "exerciseCompleted": "Exercise completed!",
    "exerciseCorrect": "Great! Your prompt works correctly.",
    "exerciseIncorrect": "Not quite right. Try refining your prompt.",
    "validationError": "Error validating your prompt.",
    "tryAgainWithHints": "Try again or check the hints for guidance.",
    "previousLesson": "Previous Lesson",
    "nextLesson": "Next Lesson",
    "practiceExercise": "Practice Exercise",
    "stage1": {
      "name": "Fundamentals",
      "description": "Master the basics of clear and effective prompt communication"
    },
    "stage2": {
      "name": "Intermediate",
      "description": "Learn structured prompting and output formatting techniques"
    },
    "stage3": {
      "name": "Advanced",
      "description": "Explore complex prompting strategies and reasoning techniques"
    },
    "stage4": {
      "name": "Practical",
      "description": "Apply prompt engineering in real-world scenarios and workflows"
    },
    "lesson1": {
      "title": "Basic Prompt Structure",
      "summary": "Learn the fundamental elements of effective prompt design and clear communication with AI systems."
    },
    "lesson2": {
      "title": "Being Clear and Direct",
      "summary": "Eliminate ambiguity and get precise responses by being specific about your requirements."
    },
    "lesson3": {
      "title": "Role Prompting",
      "summary": "Assign specific roles and perspectives to improve response quality and consistency."
    },
    "lesson4": {
      "title": "Data/Instruction Separation",
      "summary": "Use clear delimiters to separate data from instructions for better prompt clarity."
    },
    "lesson5": {
      "title": "Output Formatting",
      "summary": "Specify exact output formats to get structured, consistent responses."
    },
    "lesson6": {
      "title": "Step-by-Step Thinking",
      "summary": "Leverage chain-of-thought prompting for better reasoning and problem-solving."
    },
    "course": {
      "fundamentals": {
        "title": "Fundamentals Course",
        "summary": "Learn fundamental concepts and core principles of prompt engineering"
      },
      "intermediate": {
        "title": "Intermediate Course",
        "summary": "Master advanced techniques for data separation, output formatting, and systematic thinking"
      },
      "advanced": {
        "title": "Advanced Course",
        "summary": "Reduce hallucinations, design complex prompts, and chain tools/search for reliable workflows"
      }
    },
    "nextCourse": "Next Course",
    "integratedCourse": "Integrated Course",
    "courseOverview": "Course Overview",
    "sections": "sections",
    "previousCourse": "Previous Course",
    "clickToStartPractice": "Click the editor below to start practice",
    "learningMode": "Learning Mode",
    "playgroundMode": "Playground",
    "practiceMode": "Practice Mode",
    "learningContent": "Learning Content",
    "experimentalPlayground": "Experimental Playground",
    "interactiveExercise": "Interactive Exercise",
    "experimentWithThisPrompt": "Experiment with this prompt",
    "promptExamples": "Prompt Examples",
    "practicalExercises": "Practical Exercises",
    "previous": "Previous",
    "next": "Next",
    "learningTips": "Learning Tips",
    "tryDifferentVariations": "Try Different Prompt Variations",
    "editPrompt": "Edit Prompt",
    "systemPrompt": "System Prompt",
    "systemPromptPlaceholder": "Set AI's role and behavior rules...",
    "userPrompt": "User Prompt",
    "userPromptPlaceholder": "Enter your question or instruction...",
    "runPrompt": "Run Prompt",
    "running": "Running...",
    "aiResponse": "AI Response",
    "goodPromptQuality": "Good output quality",
    "canImprove": "Output can improve",
    "aiThinking": "AI is thinking...",
    "clickRunPrompt": "Click \"Run Prompt\" to see AI response",
    "keyTechniquesTitle": "Key Techniques",
    "commonPitfallsTitle": "Common Pitfalls",
    "promptTemplate": "Prompt Template",
    "promptTemplatePlaceholder": "Enter your prompt template with variables like {{example1}}, {{example2}}...",
    "templateHint": "Use {{format}} for placeholders that will be replaced",
    "templateVariables": "Template Variables",
    "enterValue": "Enter value...",
    "finalPrompt": "Final Prompt (Generated)",
    "finalPromptHint": "This is the final prompt after variable replacement",
    "rateLimitError": "Rate limit exceeded. Please wait a moment and try again.",
    "serviceUnavailableError": "Service temporarily unavailable. Please try again later.",
    "generalError": "An error occurred. Please try again.",
    "evaluationError": "Could not evaluate prompt quality automatically."
  },
  "courses": {
    "fundamentals": {
      "title": "Fundamentals Course",
      "summary": "Comprehensive step-by-step guide to mastering Claude's best prompt engineering practices. Learn three core skills: API structure, clear communication, and role assignment.",
      "sections": {
        "basicStructure": {
          "title": "Chapter 1: Basic Prompt Structure",
          "theory": "Learn the required parameters of the Messages API: model, max_tokens, messages array. Master the user/assistant conversation format and correct usage of system prompts.",
          "examples": [
            "Correct format: role='user', content='Hi Claude, how are you?'",
            "Wrong format: Missing role and content fields",
            "System prompt: You are a logical reasoning expert who specializes in solving complex logical problems"
          ],
          "coreConcepts": {
            "0": "**Message Alternation Rule** - User and Assistant messages MUST alternate, and messages MUST start with a User turn",
            "1": "**System Prompts** - System prompts provide context, instructions, and guidelines to AI models, structurally separate from user & assistant messages",
            "2": "**Message Formatting** - When using message APIs, insert newlines between each message, or AI will consider unseparated content as a single message",
            "3": "**Multi-turn Conversations** - You can include multiple User/Assistant pairs in a prompt, and have AI continue from an ending Assistant message",
            "4": "**Value of System Prompts** - Well-written system prompts improve AI performance, such as increasing AI's ability to follow rules and instructions"
          },
          "keyTechniques": {
            "0": "Messages API parameters: model, max_tokens, messages",
            "1": "User/Assistant conversation format",
            "2": "System prompt configuration",
            "3": "Message alternation rules"
          },
          "commonPitfalls": {
            "0": "Not alternating user/assistant messages",
            "1": "Starting with assistant message instead of user",
            "2": "Mixing system prompt with conversation messages"
          },
          "exercises": {
            "countToThree": {
              "instructions": "Use the correct user/assistant format to have Claude count to three",
              "template": "[Replace this text]",
              "hints": [
                "State your request directly",
                "Use simple clear instructions",
                "Avoid unnecessary complexity"
              ]
            },
            "systemPromptChild": {
              "instructions": "Modify the SYSTEM_PROMPT to make Claude respond like a 3-year-old child",
              "template": "SYSTEM_PROMPT = '[Replace this text]'",
              "hints": [
                "Think about how 3-year-olds speak",
                "Use simple vocabulary and expressions",
                "Add childlike language features"
              ]
            }
          }
        },
        "clearCommunication": {
          "title": "Chapter 2: Being Clear and Direct",
          "theory": "Claude needs clear instructions. Follow the golden rule: if you show your prompt to a colleague and they're confused, Claude will be confused too.",
          "examples": [
            "Vague: Write a haiku about robots",
            "Clear: Write a haiku about robots, skip the preamble and go straight to the poem",
            "Vague: Who is the best basketball player of all time?",
            "Force choice: Who is the best basketball player of all time? While there are different opinions, if you had to choose one, who would it be?"
          ],
          "coreConcepts": {
            "0": "**Clarity Principle** - Vague instructions lead to poor results, specific requirements get good output. State your needs and expectations directly",
            "1": "**Format Constraints** - Specify output format, length, style and other constraints to make AI produce results that meet expectations",
            "2": "**Colleague Test** - Show your prompt to a colleague, if they're confused, AI will be confused too",
            "3": "**Forced Choice** - Use 'if you had to choose one' to make AI avoid vague hedging and give clear answers"
          },
          "keyTechniques": {
            "0": "Direct and specific instructions",
            "1": "Format constraints and requirements",
            "2": "Forcing specific choices when needed",
            "3": "The colleague test for clarity"
          },
          "commonPitfalls": {
            "0": "Being too vague or ambiguous",
            "1": "Not specifying output format",
            "2": "Allowing AI to avoid making choices"
          },
          "exercises": {
            "spanishResponse": {
              "instructions": "Modify the SYSTEM_PROMPT to make Claude respond in Spanish",
              "template": "SYSTEM_PROMPT = '[Replace this text]'",
              "hints": [
                "Tell Claude directly which language to use",
                "You can specify language requirements in the system prompt"
              ]
            },
            "onePlayerOnly": {
              "instructions": "Have Claude respond with only one player's name, no other text or punctuation",
              "template": "[Replace this text]",
              "hints": [
                "Be very specific about format requirements",
                "Clearly state no explanations or other content",
                "Force Claude to make a choice"
              ]
            },
            "longStory": {
              "instructions": "Write a story of 800+ words",
              "template": "[Replace this text]",
              "hints": [
                "Clearly specify story length",
                "You can provide specific story themes",
                "Request detailed descriptions and plot development"
              ]
            }
          }
        },
        "rolePrompting": {
          "title": "Chapter 3: Assigning Roles (Role Prompting)",
          "theory": "Assigning specific roles to Claude can significantly improve performance. Role setting can change response style, tone, and level of expertise.",
          "examples": [
            "No role: What about skateboarding?",
            "Cat role: system='You are a cat' will answer from a cat's perspective",
            "Logic error: Jack looks at Anne, Anne looks at George, Jack is married George is unmarried, is a married person looking at an unmarried person?",
            "Logic expert: system='You are a logical reasoning expert' correctly analyzes this problem"
          ],
          "coreConcepts": {
            "0": "**Role Context** - Claude only knows what you tell it, so provide complete role background and context",
            "1": "**Role Effects** - Role setting can change response style, tone, and expertise level, improving performance in specific domains",
            "2": "**Setting Location** - Roles can be set in system prompts or user messages, both methods are effective",
            "3": "**Detail Importance** - The more detailed the role description, the better the effect. Include specific professional background and behavioral expectations"
          },
          "keyTechniques": {
            "0": "Assigning specific roles and personas",
            "1": "Providing detailed role context",
            "2": "Using roles to improve domain expertise",
            "3": "Setting roles in system prompts"
          },
          "commonPitfalls": {
            "0": "Using roles without sufficient context",
            "1": "Not being specific about role expertise",
            "2": "Forgetting to set appropriate tone with role"
          },
          "exercises": {
            "mathCorrection": {
              "instructions": "Have Claude find the math error: 2x-3=9, 2x=6, x=3",
              "template": "Is this equation solved correctly?\n\n2x - 3 = 9\n2x = 6\nx = 3",
              "hints": [
                "Assign Claude a math teacher or logic expert role",
                "Request careful checking of each step",
                "Emphasize finding the error"
              ]
            }
          }
        }
      }
    },
    "intermediate": {
      "title": "Intermediate Prompt Engineering",
      "summary": "Master advanced techniques for data separation, output formatting, and systematic thinking",
      "sections": {
        "dataInstructions": {
          "title": "Chapter 4: Separating Data from Instructions",
          "theory": "Learn how to create reusable prompt templates by cleanly separating instructions from variable data. This chapter covers the use of XML tags, f-string templating, and best practices for handling user input safely and effectively.",
          "examples": [
            "Create dynamic prompts with variable substitution using f-strings",
            "Use XML tags to clearly separate data from instructions",
            "Handle messy or unclear input by providing proper boundaries"
          ],
          "keyTechniques": {
            "0": "F-string templating with variables",
            "1": "XML tag wrapping for data boundaries",
            "2": "Clear instruction-data separation",
            "3": "Input validation patterns"
          },
          "commonPitfalls": {
            "0": "Not clearly separating instructions from data",
            "1": "Forgetting to wrap variable content in XML tags",
            "2": "Using unclear variable names in templates"
          }
        },
        "formatting": {
          "title": "Chapter 5: Formatting Output & Speaking for Claude",
          "theory": "Master techniques for controlling and formatting AI output. Learn how to use XML tags, JSON formatting, and prefilling to get exactly the response format you need.",
          "examples": [
            "Format output in XML tags for easy parsing",
            "Use prefilling to guide response generation",
            "Enforce JSON output for structured data"
          ],
          "keyTechniques": {
            "0": "XML tag output formatting",
            "1": "Assistant turn prefilling",
            "2": "JSON output enforcement",
            "3": "Multi-variable template formatting"
          },
          "commonPitfalls": {
            "0": "Not using consistent output format tags",
            "1": "Forgetting to prefill the opening tag",
            "2": "Not enforcing format requirements"
          }
        },
        "thinking": {
          "title": "Chapter 6: Precognition (Thinking Step by Step)",
          "theory": "Improve AI accuracy by implementing chain-of-thought prompting. Learn how to structure reasoning processes and use step-by-step thinking for complex problems.",
          "examples": [
            "Guide AI through explicit reasoning steps",
            "Use XML-tagged thinking sections for organization",
            "Apply multi-perspective analysis for better decisions"
          ],
          "keyTechniques": {
            "0": "Explicit reasoning steps",
            "1": "XML-tagged thinking sections",
            "2": "Multi-perspective analysis",
            "3": "Brainstorming before answering"
          },
          "commonPitfalls": {
            "0": "Asking for thinking without showing the work",
            "1": "Not structuring the thinking process",
            "2": "Skipping reasoning for complex problems"
          }
        },
        "examples": {
          "title": "Chapter 7: Using Examples (Few-Shot Prompting)",
          "theory": "Leverage the power of examples to guide AI behavior. Learn how to select effective examples, format them properly, and use few-shot prompting for consistent results.",
          "examples": [
            "Choose optimal examples for guidance",
            "Format examples for maximum impact",
            "Apply examples across different domains"
          ],
          "keyTechniques": {
            "0": "Few-shot example selection",
            "1": "Example formatting patterns",
            "2": "Context-appropriate examples",
            "3": "Output format consistency"
          },
          "commonPitfalls": {
            "0": "Using poor quality or irrelevant examples",
            "1": "Not maintaining consistent format across examples",
            "2": "Too few or too many examples"
          }
        }
      }
    },
    "advanced": {
      "title": "Advanced Prompt Engineering",
      "summary": "Defend against hallucinations, author bespoke prompt blueprints, and orchestrate iterative, tool-aware workflows.",
      "sections": {
        "avoidingHallucinations": {
          "title": "Chapter 8: Avoiding Hallucinations",
          "theory": "Claude wants to be helpful and will fabricate details when pressured. This chapter shows how to give the model explicit permission to abstain, how to make it collect evidence before answering, and how to tune parameters so accuracy beats creativity. Combine these steps with clear structure so every factual reply is grounded in verifiable text.",
          "examples": {
            "0": "Add a rule such as “If you are not certain, reply with ‘I don't know’ and explain what would be needed to know.”",
            "1": "Split the task: first extract supporting sentences inside 〈evidence〉 tags, then answer using only those sentences.",
            "2": "Lower temperature or ask Claude to double-check numbers before finalizing the response."
          },
          "keyTechniques": {
            "0": "Give Claude an explicit out so refusing is preferable to guessing.",
            "1": "Force an evidence-first workflow with XML tags or numbered steps.",
            "2": "Require citations or quote IDs in every factual answer.",
            "3": "Control randomness (temperature, top_p) when accuracy matters."
          },
          "commonPitfalls": {
            "0": "Burying the user question above distractor text without restating it at the end.",
            "1": "Letting Claude answer without quoting the provided document.",
            "2": "Failing to explain what to do when no evidence is found."
          }
        },
        "complexPrompts": {
          "title": "Chapter 9: Complex Prompts from Scratch",
          "theory": "Complex prompts are blueprints. They start with role and mission, add context, guardrails, and workflow steps, then pin down output format and success criteria. By composing these sections deliberately you can recycle the template across industries while still sounding bespoke.",
          "examples": [
            "Wrap the persona, mission, and voice guidance inside labeled sections such as 〈role〉, 〈mission〉, and 〈tone〉.",
            "List guardrails and forbidden behaviors before giving task steps.",
            "Finish with structured output requirements so reviewers know what to expect."
          ],
          "keyTechniques": {
            "0": "Break prompts into reusable sections: role, goals, context, workflow, output.",
            "1": "Use XML tags or markdown headings so each section is easy to scan.",
            "2": "Describe failure modes and escalation paths in the guardrails section.",
            "3": "Parameterize the template with placeholder tokens (e.g., [[INDUSTRY]]) so you can reuse it across verticals."
          },
          "commonPitfalls": {
            "0": "Writing one giant paragraph with mixed instructions and tone.",
            "1": "Skipping guardrails, which lets Claude overstep or guess when unsure.",
            "2": "Forgetting to specify how and when the conversation should end."
          }
        },
        "chaining": {
          "title": "Appendix 10.1: Prompt Chaining",
          "theory": "Prompt chaining has Claude critique or refine its own work before answering the user. Each pass has a clear purpose—brainstorm, evaluate, revise—and the output of one pass becomes the input of the next. This mirrors how humans rewrite drafts to remove mistakes.",
          "examples": [
            "Ask Claude to generate candidates, then run a second pass that scores them before choosing one.",
            "Keep the draft reasoning inside 〈analysis〉 tags and reserve 〈final〉 for the user-facing response.",
            "Request a “quality review” step that checks reasoning, math, or policy compliance before publishing the answer."
          ],
          "keyTechniques": {
            "0": "Name every stage (brainstorm, verify, final) so Claude knows when to switch modes.",
            "1": "Carry over the relevant context between stages explicitly.",
            "2": "Provide rubrics for the review pass so it knows what to look for.",
            "3": "Stop the chain after a fixed number of passes to avoid infinite loops."
          },
          "commonPitfalls": {
            "0": "Letting Claude reuse the first-pass answer verbatim without critique.",
            "1": "Not explaining what counts as an error during the review step.",
            "2": "Failing to reset temperature or tone between stages."
          }
        },
        "toolsRetrieval": {
          "title": "Appendix 10.2–10.3: Tool Use, Search & Retrieval",
          "theory": "Tools and retrieval give Claude grounded data and capabilities it does not inherently possess. You describe the tool interface, have Claude emit structured calls, feed the tool results back, and require the final answer to cite what it read. The same pattern powers calculator calls, database lookups, and RAG systems.",
          "examples": [
            "Define a JSON schema for function calls (name + arguments) and tell Claude when to emit them.",
            "After a tool call, paste the tool output back into the prompt using a delimiter such as 〈tool_result id=\"1\"〉…〈/tool_result〉 before asking for the final answer.",
            "Run a miniature RAG flow: generate search queries, summarize hits, then synthesize a grounded response with citations."
          ],
          "keyTechniques": {
            "0": "Document each tool's purpose, arguments, and limits inside the prompt.",
            "1": "Tag tool outputs (〈tool_result id=\"1\"〉...〈/tool_result〉) so Claude cannot confuse them with instructions.",
            "2": "Instruct Claude to only answer with information present in the retrieved snippets.",
            "3": "Log citations or IDs so downstream systems can audit the answer."
          },
          "commonPitfalls": {
            "0": "Leaving tool invocation rules implicit, which causes malformed calls.",
            "1": "Letting Claude invent tool responses instead of waiting for real data.",
            "2": "Skipping the synthesis step after retrieval, leading to copy-pasted snippets."
          }
        }
      }
    }
  },
  "practiceExercises": {
    "fundamentals": {
      "chapter1BasicStructure": {
        "title": "Chapter 1 Exercise: Count to Three (API Structure)",
        "description": "Learn the correct Messages API format. This is the most basic exercise to ensure you understand how to construct effective prompts.",
        "expectedOutput": "A response containing 1, 2, 3 with clear formatting.",
        "hints": {
          "0": "State your requirement directly: 'Count to three'",
          "1": "Use simple, clear instructions",
          "2": "Avoid unnecessary complexity"
        },
        "variations": {
          "directInstruction": {
            "name": "Direct Instruction",
            "explanation": "The simplest, most direct approach"
          },
          "clearFormat": {
            "name": "Clear Format",
            "explanation": "Specifies the output format"
          }
        }
      },
      "chapter1SystemPrompt": {
        "title": "Chapter 1 Exercise: 3-Year-Old Role (System Prompt)",
        "description": "Experience the power of system prompts. Through role assignment, completely change Claude's response style.",
        "expectedOutput": "An excited response like a 3-year-old child, possibly with laughter, simple vocabulary, and innocent expressions.",
        "hints": {
          "0": "Think about how a 3-year-old talks: excited, curious, simple vocabulary",
          "1": "You can add childish expressions like 'soo big' or 'giggles'",
          "2": "The system prompt defines the AI's entire 'personality'"
        },
        "variations": {
          "noSystemPrompt": {
            "name": "No System Prompt",
            "explanation": "No system prompt set, see the default response"
          },
          "strictScientist": {
            "name": "Strict Scientist",
            "explanation": "System: 'You are a strict physicist. Give precise scientific answers.'"
          },
          "friendlyTeacher": {
            "name": "Friendly Teacher",
            "explanation": "System: 'You are a friendly teacher explaining to children.'"
          }
        }
      },
      "chapter2Spanish": {
        "title": "Chapter 2 Exercise: Spanish Response (Clear Instructions)",
        "description": "Learn how to give clear instructions to control output language. This exercise shows that the API can precisely follow your requirements.",
        "expectedOutput": "A response in Spanish, including Spanish greetings like 'Hola'.",
        "hints": {
          "0": "Directly state the language requirement in the prompt",
          "1": "You can say 'Please respond in Spanish' or 'Responde en español'",
          "2": "Observe how AI completely switches languages"
        },
        "variations": {
          "frenchResponse": {
            "name": "French Response",
            "explanation": "Request response in French"
          },
          "bilingualResponse": {
            "name": "Bilingual Response",
            "explanation": "Request response in both English and Spanish"
          }
        }
      },
      "chapter2Basketball": {
        "title": "Chapter 2 Exercise: Just One Name (Precise Format)",
        "description": "Learn how to give extremely specific format requirements. This exercise teaches you how to get precisely controlled output.",
        "expectedOutput": "Just one player's name, no other text or explanation.",
        "hints": {
          "0": "Be very specific about format requirements",
          "1": "Clearly state no explanations or other content",
          "2": "Force AI to make a choice"
        },
        "variations": {
          "withExplanation": {
            "name": "With Explanation",
            "explanation": "Allow Claude to explain the choice"
          },
          "top3List": {
            "name": "Top 3 List",
            "explanation": "Request top 3 players instead of one"
          },
          "specificFormat": {
            "name": "Specific Format",
            "explanation": "Request name with one reason"
          }
        }
      },
      "chapter2LongStory": {
        "title": "Chapter 2 Exercise: Write Long Story (Clear Requirements)",
        "description": "Learn how to control output length through clear requirements. This exercise demonstrates the importance of providing specific metrics.",
        "expectedOutput": "A story of at least 800 words about a robot learning to paint, with rich plot and detailed descriptions.",
        "hints": {
          "0": "Clearly request story length: at least 800 words",
          "1": "Provide specific story topic and theme",
          "2": "Request detailed descriptions and plot development",
          "3": "Can request inclusion of dialogue and scene descriptions"
        },
        "variations": {
          "noLength": {
            "name": "No Length Requirement",
            "explanation": "Don't specify length, see default story length"
          },
          "exactWordCount": {
            "name": "Exact Word Count",
            "explanation": "Specify exact word count and request specific elements"
          },
          "longerStory": {
            "name": "Longer Story",
            "explanation": "Request longer length and more complex structure"
          }
        }
      },
      "chapter3MathLogic": {
        "title": "Chapter 3 Exercise: Math Error Check (Role Assignment)",
        "description": "Experience how role assignment affects logical reasoning. By assigning Claude an expert role, improve its performance in specific domains.",
        "expectedOutput": "Find the math error: The step from 2x-3=9 to 2x=6 is wrong, it should be 2x=12.",
        "hints": {
          "0": "Assign Claude a math teacher or logic expert role",
          "1": "Request careful checking of each calculation step",
          "2": "Emphasize finding where the error is"
        },
        "variations": {
          "noRole": {
            "name": "No Role Set",
            "explanation": "No specific role set, see basic performance"
          },
          "logicExpert": {
            "name": "Logic Expert",
            "explanation": "Set as logic reasoning expert, focus on logical analysis"
          },
          "studentRole": {
            "name": "Student Role",
            "explanation": "Set as a student learning math, show thinking process"
          }
        }
      }
    },
    "intermediate": {
      "chapter4Haiku": {
        "title": "Chapter 4 Practice: Haiku Template Exercise",
        "description": "Create a reusable prompt template for generating haikus about any topic",
        "expectedOutput": "A haiku that mentions the specified topic",
        "hints": {
          "0": "Use {{TOPIC}} as a placeholder in your prompt template",
          "1": "Make sure to ask for a haiku format",
          "2": "Test with different topics to verify the template works"
        },
        "variations": {
          "simple": {
            "name": "Simple Prompt",
            "explanation": "Direct approach without template variables"
          },
          "template": {
            "name": "Template Approach",
            "explanation": "Using variable substitution for reusability"
          }
        }
      },
      "chapter4DogXML": {
        "title": "Chapter 4 Practice: XML Tag Separation",
        "description": "Fix a messy prompt by using XML tags to separate the question from surrounding text",
        "expectedOutput": "A clear answer about dogs being brown",
        "hints": {
          "0": "Wrap the actual question in XML tags",
          "1": "This helps AI focus on what's important",
          "2": "Compare the results with and without XML tags"
        },
        "variations": {
          "noXML": {
            "name": "Without XML Tags",
            "explanation": "See how messy text confuses the AI"
          },
          "clearFormat": {
            "name": "Clean Format",
            "explanation": "Well-structured prompt with clear boundaries"
          }
        }
      },
      "chapter5Curry": {
        "title": "Chapter 5 Practice: Response Prefilling",
        "description": "Use prefilling to guide Claude toward a specific basketball player",
        "expectedOutput": "An argument for Stephen Curry as the best player",
        "hints": {
          "0": "Start Claude's response with partial text about Curry",
          "1": "This technique 'speaks for Claude' to guide direction",
          "2": "The prefill should feel natural to continue"
        },
        "variations": {
          "noPrefill": {
            "name": "No Prefilling",
            "explanation": "Standard response without guidance"
          },
          "curryPrefill": {
            "name": "Curry Prefill",
            "explanation": "Guide response toward Stephen Curry"
          }
        }
      },
      "chapter5TwoHaikus": {
        "title": "Chapter 5 Practice: Multiple Output Formatting",
        "description": "Format output to contain two separate haikus with clear boundaries",
        "expectedOutput": "Two distinct haikus about cats, each in its own tags",
        "hints": {
          "0": "Ask for two haikus explicitly",
          "1": "Use XML tags to separate each haiku",
          "2": "Make sure the format is consistent"
        },
        "variations": {
          "oneHaiku": {
            "name": "Single Haiku",
            "explanation": "Standard single haiku request"
          },
          "numbered": {
            "name": "Numbered Format",
            "explanation": "Use numbered tags for organization"
          }
        }
      },
      "chapter6EmailClass": {
        "title": "Chapter 6 Practice: Step-by-Step Email Classification",
        "description": "Classify emails by having AI think through the process step by step",
        "expectedOutput": "Classification with visible reasoning process",
        "hints": {
          "0": "Ask AI to show its thinking process",
          "1": "Use XML tags to organize the reasoning",
          "2": "Think through each category systematically"
        },
        "variations": {
          "noThinking": {
            "name": "Direct Classification",
            "explanation": "Immediate classification without reasoning"
          },
          "detailedThinking": {
            "name": "Detailed Analysis",
            "explanation": "Comprehensive step-by-step analysis"
          }
        }
      },
      "chapter6EmailFormatted": {
        "title": "Chapter 6 Practice: Formatted Classification Output",
        "description": "Get classification results in a specific XML format",
        "expectedOutput": "Classification letter wrapped in answer tags",
        "hints": {
          "0": "Specify exact output format with XML tags",
          "1": "Ask for just the letter, nothing else",
          "2": "Use prefilling if needed to enforce format"
        },
        "variations": {
          "withExplanation": {
            "name": "With Explanation",
            "explanation": "Include reasoning with classification"
          },
          "xmlOnly": {
            "name": "XML Format Only",
            "explanation": "Strict format requirements"
          }
        }
      },
      "chapter7EmailExamples": {
        "title": "Chapter 7 Practice: Few-Shot Email Classification",
        "description": "Use examples to teach AI how to classify emails consistently",
        "expectedOutput": "Correct classification following example patterns",
        "hints": {
          "0": "Provide 2-3 clear examples of classifications",
          "1": "Show the exact format you want",
          "2": "Include examples for different categories"
        },
        "variations": {
          "noExamples": {
            "name": "No Examples",
            "explanation": "Classification without guidance examples"
          },
          "moreExamples": {
            "name": "Extended Examples",
            "explanation": "Multiple examples for better guidance"
          }
        }
      }
    },
    "advanced": {
      "chapter8GiveAnOut": {
        "title": "Chapter 8 Exercise: Give Claude an Out",
        "description": "Rewrite the prompt so Claude can admit uncertainty about Beyoncé's discography and must cite a source before answering.",
        "expectedOutput": "A numbered answer that either cites a reliable source for the album count or clearly says “I don't know” with justification.",
        "hints": {
          "0": "Add an explicit rule that refusing to answer is better than guessing.",
          "1": "Request citations or supporting quotes before the final sentence.",
          "2": "Lower temperature or emphasize accuracy over creativity."
        },
        "variations": {
          "noOut": {
            "name": "No Guardrail",
            "explanation": "The original single-shot question that often produces a hallucinated number."
          },
          "allowUnknown": {
            "name": "Allow Unknown",
            "explanation": "Shows how the response changes when Claude can say it does not know."
          }
        }
      },
      "chapter8CiteEvidence": {
        "title": "Chapter 8 Exercise: Quote the Prospectus",
        "description": "Force Claude to extract relevant lines from the Matterport prospectus before answering the subscriber-growth question.",
        "expectedOutput": "Claude lists the supporting quote (showing subscribers up 49×) and then answers using that quote as evidence.",
        "hints": {
          "0": "Separate the evidence-gathering and answering steps.",
          "1": "Wrap quotes in 〈evidence〉 tags or bullets so they stand out.",
          "2": "Explain what to do if the document does not contain the answer."
        },
        "variations": {
          "noQuotes": {
            "name": "No Evidence Step",
            "explanation": "Claude reads the document but can still guess without citing it."
          },
          "quoteFirst": {
            "name": "Quote-First Template",
            "explanation": "Claude must list relevant numbers before forming its conclusion."
          }
        }
      },
      "chapter9Blueprint": {
        "title": "Chapter 9 Exercise: Prompt Blueprint",
        "description": "Fill in the blueprint template for a friendly career coach, including role, goals, guardrails, workflow, and output format.",
        "expectedOutput": "A multi-section prompt with headings or XML tags that can be reused across coaching scenarios.",
        "hints": {
          "0": "Start with persona and mission so Claude knows who it is before acting.",
          "1": "State guardrails and escalation paths explicitly.",
          "2": "Close with precise output structure (tags, JSON, or markdown headings)."
        },
        "variations": {
          "generic": {
            "name": "Generic Prompt",
            "explanation": "Unstructured instructions that feel bland and inconsistent."
          },
          "structured": {
            "name": "Structured Blueprint",
            "explanation": "Uses labeled sections for role, workflow, and output."
          }
        }
      },
      "chapter10Chaining": {
        "title": "Chapter 10 Exercise: Chain + Tool",
        "description": "Write instructions that make Claude brainstorm, call a calculator tool, and then deliver a verified answer in a final section.",
        "expectedOutput": "Claude produces 〈analysis〉 thinking, emits a calculator call, consumes the tool result, and responds inside 〈final〉 tags.",
        "hints": {
          "0": "Explain exactly when to call the tool and what arguments to send.",
          "1": "Use XML tags or headings to separate analysis from the user-facing answer.",
          "2": "Require Claude to report if verification fails or data is missing."
        },
        "variations": {
          "singleShot": {
            "name": "Single-Shot Answer",
            "explanation": "Claude answers immediately with no verification."
          },
          "chainAndTool": {
            "name": "Chain + Tool Pattern",
            "explanation": "Claude reasons, calls the tool, and only then replies to the user."
          }
        }
      }
    }
  },
  "playground": {
    "fundamentals": {
      "basicStructure": {
        "title": "Chapter 1 Playground: Basic Structure & System Prompts",
        "creativeCounting": {
          "name": "Creative Counting",
          "description": "Experiment with different ways to count - try making it fun!",
          "variations": {
            "basic": "Basic Creative",
            "basicExplanation": "Simple creative counting approach",
            "storytelling": "Story Format",
            "storytellingExplanation": "Count by embedding numbers in a narrative",
            "poetic": "Poetic Style",
            "poeticExplanation": "Use rhymes and poetic language for counting",
            "withContext": "Educational Context",
            "withContextExplanation": "Add educational context with system prompt"
          }
        },
        "systemPromptExperiment": {
          "name": "System Prompt Structure",
          "description": "Learn how system prompts guide AI behavior and response style",
          "variations": {
            "basic": "No System Prompt",
            "basicExplanation": "Standard AI response without system guidance",
            "structured": "Structured Request",
            "structuredExplanation": "System prompt requesting clear structure",
            "detailed": "Detailed Instructions",
            "detailedExplanation": "System prompt asking for detailed scientific explanation",
            "simple": "Simplicity Request",
            "simpleExplanation": "System prompt requesting simple language"
          }
        },
        "hint1": "Try different creative approaches to basic tasks",
        "hint2": "Experiment with various system prompt structures",
        "hint3": "Notice how system prompts change AI response style"
      },
      "clearCommunication": {
        "title": "Chapter 2 Playground: Clear Communication",
        "languageExperiment": {
          "name": "Language Choice",
          "description": "Explore how AI chooses languages and explains decisions",
          "variations": {
            "basic": "Open Choice",
            "basicExplanation": "AI chooses any language and explains why",
            "specific": "Specific Language",
            "specificExplanation": "Request a specific language with cultural context",
            "comparative": "Language Comparison",
            "comparativeExplanation": "Compare greetings across multiple languages",
            "contextual": "Business Context",
            "contextualExplanation": "Professional greeting with business context"
          }
        },
        "formatExperiment": {
          "name": "Format Styles",
          "description": "Try different formatting approaches for lists and structure",
          "variations": {
            "basic": "Flexible Format",
            "basicExplanation": "AI chooses formatting style freely",
            "numbered": "Numbered List",
            "numberedExplanation": "Specific numbered list with explanations",
            "detailed": "Detailed Descriptions",
            "detailedExplanation": "Rich descriptions with reasons",
            "creative": "Creative Format",
            "creativeExplanation": "Creative presentation like poem or story"
          }
        },
        "hint1": "Be specific about your requirements",
        "hint2": "Try different output formats",
        "hint3": "Experiment with explicit vs implicit instructions"
      },
      "rolePrompting": {
        "title": "Chapter 3 Playground: Role Prompting",
        "roleComparison": {
          "name": "Role Comparison",
          "description": "Compare responses from different professional perspectives",
          "variations": {
            "basic": "No Role Context",
            "basicExplanation": "General AI response without specific professional role",
            "conservative": "Conservative Advisor",
            "conservativeExplanation": "Risk-averse financial advisor perspective",
            "aggressive": "Tech-Savvy Advisor",
            "aggressiveExplanation": "Digital asset specialist with growth focus",
            "balanced": "Certified Planner",
            "balancedExplanation": "Objective analysis from certified financial planner"
          }
        },
        "expertiseExperiment": {
          "name": "Expertise Levels",
          "description": "See how different expertise levels affect explanations",
          "variations": {
            "basic": "General Response",
            "basicExplanation": "Standard AI explanation without expertise role",
            "professor": "Academic Professor",
            "professorExplanation": "Data science professor making topics accessible",
            "beginner": "Beginner Tutor",
            "beginnerExplanation": "Tutor specialized in teaching absolute beginners",
            "practical": "Industry Expert",
            "practicalExplanation": "Working data scientist showing real applications"
          }
        },
        "hint1": "Try contrasting professional roles",
        "hint2": "Experiment with different expertise levels",
        "hint3": "Notice how role context shapes responses"
      }
    },
    "intermediate": {
      "dataInstructionSeparation": {
        "title": "Chapter 4 Playground: Data & Instruction Separation",
        "examples": {
          "animalSounds": {
            "name": "Animal Sound Generator",
            "description": "Create a template for generating animal sounds",
            "variations": {
              "withXML": {
                "name": "With XML Tags",
                "explanation": "Clear separation using XML boundaries"
              },
              "template": {
                "name": "Template Variables",
                "explanation": "Using variable substitution patterns"
              }
            }
          },
          "emailRewrite": {
            "name": "Email Rewriting Template",
            "description": "Template for rewriting emails in different styles",
            "variations": {
              "noXML": {
                "name": "Without XML",
                "explanation": "See how unclear boundaries affect results"
              },
              "formal": {
                "name": "Formal Style",
                "explanation": "Professional email rewriting template"
              }
            }
          }
        },
        "hints": {
          "0": "Use XML tags to clearly separate data from instructions",
          "1": "Create reusable templates with variable placeholders",
          "2": "Test templates with different inputs to verify consistency"
        }
      },
      "outputFormatting": {
        "title": "Chapter 5 Playground: Output Formatting Control",
        "examples": {
          "haikuXML": {
            "name": "XML-Formatted Haikus",
            "description": "Control haiku output format with XML tags",
            "variations": {
              "prefilled": {
                "name": "Prefilled Opening",
                "explanation": "Use prefilling to start the XML tag"
              },
              "json": {
                "name": "JSON Format",
                "explanation": "Structure haiku as JSON object"
              }
            }
          },
          "emailStyle": {
            "name": "Styled Email Output",
            "description": "Format email rewrites with dynamic XML tags",
            "variations": {
              "formal": {
                "name": "Formal Style",
                "explanation": "Professional email formatting"
              },
              "casual": {
                "name": "Casual Style",
                "explanation": "Informal email formatting"
              }
            }
          }
        },
        "hints": {
          "0": "Use XML tags to structure output clearly",
          "1": "Combine prefilling with format specifications",
          "2": "Experiment with different output structures"
        }
      },
      "stepByStepThinking": {
        "title": "Chapter 6 Playground: Chain-of-Thought Reasoning",
        "examples": {
          "movieReview": {
            "name": "Movie Review Analysis",
            "description": "Analyze sentiment with step-by-step reasoning",
            "variations": {
              "noThinking": {
                "name": "Direct Analysis",
                "explanation": "Immediate sentiment judgment"
              },
              "brainstorm": {
                "name": "Brainstorming Process",
                "explanation": "Show detailed thinking process"
              }
            }
          },
          "factChecking": {
            "name": "Fact Verification",
            "description": "Verify facts through systematic reasoning",
            "variations": {
              "direct": {
                "name": "Direct Answer",
                "explanation": "Immediate response without research"
              },
              "detailed": {
                "name": "Research Process",
                "explanation": "Show research and verification steps"
              }
            }
          }
        },
        "hints": {
          "0": "Ask AI to show its reasoning process explicitly",
          "1": "Use XML tags to organize thinking steps",
          "2": "Compare results with and without step-by-step analysis"
        }
      },
      "fewShotExamples": {
        "title": "Chapter 7 Playground: Example-Driven Prompting",
        "examples": {
          "parentBot": {
            "name": "Parent Response Bot",
            "description": "Use examples to create consistent parenting responses",
            "variations": {
              "noExample": {
                "name": "Role-Based Only",
                "explanation": "Using system prompt without examples"
              },
              "multipleExamples": {
                "name": "Multiple Examples",
                "explanation": "Several examples for better consistency"
              }
            }
          },
          "dataExtraction": {
            "name": "Structured Data Extraction",
            "description": "Extract information using example formatting",
            "variations": {
              "noFormat": {
                "name": "Unstructured Output",
                "explanation": "Free-form data extraction"
              },
              "jsonFormat": {
                "name": "JSON Structure",
                "explanation": "Structured JSON output format"
              }
            }
          }
        },
        "hints": {
          "0": "Choose examples that represent the desired output clearly",
          "1": "Use 2-3 examples for best results",
          "2": "Maintain consistent formatting across all examples"
        }
      }
    },
    "advanced": {
      "hallucinationGuardrails": {
        "title": "Hallucination Guardrails",
        "hints": {
          "0": "Give Claude literal instructions for how to respond when evidence is missing.",
          "1": "Bundle “find evidence” and “answer” into separate, clearly labeled steps.",
          "2": "Use low temperature or system prompts that prioritize accuracy over creativity."
        },
        "examples": {
          "allowUnknown": {
            "name": "Allow Unknown Responses",
            "description": "Practice adding refusal options and fallback language to fact-heavy questions.",
            "variations": {
              "temperature": {
                "name": "Temperature Guardrail",
                "explanation": "Demonstrates how lowering randomness stabilizes factual answers."
              },
              "quoteFirst": {
                "name": "Quote-First Flow",
                "explanation": "Requires Claude to surface evidence before composing the answer."
              }
            }
          }
        }
      },
      "complexBlueprints": {
        "title": "Complex Prompt Blueprints",
        "hints": {
          "0": "Keep sections short and labeled so stakeholders can edit them.",
          "1": "Document guardrails next to the workflow so Claude always sees the constraints.",
          "2": "Template variables ({{ROLE}}, {{QUESTION}}) make the prompt reusable."
        },
        "examples": {
          "careerCoach": {
            "name": "Career Coach Blueprint",
            "description": "A friendly coaching template that highlights persona, workflow, and output formatting.",
            "variations": {
              "brief": {
                "name": "Brief Prompt",
                "explanation": "Quick instructions without structure for comparison."
              },
              "blueprint": {
                "name": "Full Blueprint",
                "explanation": "Includes labeled sections and placeholders."
              }
            }
          },
          "auditBot": {
            "name": "Compliance Audit Bot",
            "description": "A risk/compliance template that enforces step-by-step checks and JSON output.",
            "variations": {
              "linear": {
                "name": "Linear Instructions",
                "explanation": "Single paragraph without tags."
              },
              "sectioned": {
                "name": "Sectioned Template",
                "explanation": "Shows how 〈checks〉, 〈steps〉, and 〈format〉 blocks keep the audit workflow organized."
              }
            }
          }
        }
      },
      "workflowOrchestration": {
        "title": "Workflow Orchestration",
        "hints": {
          "0": "Label each phase (analysis, tool call, final) so you can debug the flow.",
          "1": "Describe tool schemas and provide placeholders for arguments.",
          "2": "Ground final answers in retrieved snippets with citations."
        },
        "examples": {
          "promptChaining": {
            "name": "Prompt Chaining Pattern",
            "description": "Have Claude think, verify with a tool, then publish the response.",
            "variations": {
              "singleStep": {
                "name": "Single Step",
                "explanation": "Baseline with no chaining to compare against."
              },
              "multiTool": {
                "name": "Multi-Tool Flow",
                "explanation": "Includes analysis, tool request, and final format placeholders."
              }
            }
          },
          "searchRetrieval": {
            "name": "Search & Retrieval",
            "description": "Generate search queries, summarize hits, then synthesize a grounded answer.",
            "variations": {
              "noGrounding": {
                "name": "No Grounding",
                "explanation": "Shows what happens when you answer without retrieved context."
              },
              "withCitations": {
                "name": "Grounded with Citations",
                "explanation": "Adds 〈cite〉 tags or snippet IDs to every fact so the synthesized answer stays grounded."
              }
            }
          }
        }
      }
    }
  },
  "rateLimitError": "Rate limit exceeded. Please wait a moment and try again.",
  "serviceUnavailableError": "Service temporarily unavailable. Please try again later.",
  "generalError": "An error occurred while processing your request. Please try again.",
  "evaluationError": "Failed to evaluate prompt quality. Please try again."
}
